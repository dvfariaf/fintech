{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: (18 june)\n",
    "Check Roadmap!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-08 23:35:52,878 - fintech - INFO - Program started\n",
      "2018-03-08 23:35:52,892 - fintech - INFO - Done!\n"
     ]
    }
   ],
   "source": [
    "# log_with_config.py\n",
    "import logging\n",
    "import logging.config\n",
    " \n",
    "\"\"\"\n",
    "Based on http://docs.python.org/howto/logging.html#configuring-logging\n",
    "\"\"\"\n",
    "logging.config.fileConfig('logs/logging.conf')\n",
    "logger = logging.getLogger(\"fintech\")\n",
    "\n",
    "logger.info(\"WELCOME TO FINTECH! - Program started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named PyPDF2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-64846f3a3f15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mPyPDF2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPdfFileReader\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtabula\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named PyPDF2"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfFileReader\n",
    "import re\n",
    "import tabula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining functions\n",
    "\n",
    "def convert_amount(df, variable, errors = \"raise\"):   \n",
    "    df[variable + \"_tmp\"] = df[variable].str.replace(\"*\", \"\")\n",
    "    df[variable + \"_tmp\"] = df[variable + \"_tmp\"].str.replace(\".\", \"\")\n",
    "    df[variable + \"_tmp\"] = df[variable + \"_tmp\"].str.replace(\",\", \".\")\n",
    "    df[variable + \"_tmp_int\"] = pd.to_numeric(df[variable + \"_tmp\"], errors = errors)\n",
    "    return df\n",
    "\n",
    "def convert_PDFunit(measure, unit=\"inches\"):\n",
    "    if unit==\"inches\":\n",
    "        return measure*72\n",
    "    elif unit==\"cm\":\n",
    "        return measure*28.3\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regular_columns = [\"date\", \"value\", \"operation_details\", \"debit\", \"credit\"]\n",
    "\n",
    "def count_pages(file_path):\n",
    "    f = open(file_path, mode='rb' )\n",
    "    try:\n",
    "        reader = PdfFileReader(f)\n",
    "        n_pages = reader.getNumPages() \n",
    "    finally:\n",
    "        f.close()\n",
    "    return n_pages\n",
    "\n",
    "def parse_pdf(file_path, page=\"all\", area=None, guess=True, columns=None):\n",
    "    # Read pdf into DataFrame          \n",
    "    if pdf_pages!=\"all\":\n",
    "        print(\"\\t\\t[INFO] Parsing page \" + str(page))\n",
    "        \n",
    "    if area is not None:\n",
    "        guess = False\n",
    "    else:\n",
    "        print(\"\\t\\t\\t[WARNING] Parsing guessing table zone, check results.\")\n",
    "    \n",
    "    df = tabula.read_pdf(file_path, \n",
    "                         output_format='dataframe',\n",
    "                         encoding = \"ISO-8859-1\",\n",
    "                         java_options=None, \n",
    "                         pandas_options = {'header' : None},\n",
    "                         multiple_tables=False, \n",
    "                         nospreadsheet = True,\n",
    "                         guess = guess,\n",
    "                         pages = page,\n",
    "                         area = area,\n",
    "                         columns = columns\n",
    "                        )\n",
    "    if df is not None:\n",
    "        df = clean_df(df, file_path)\n",
    "    else:\n",
    "        print(\"\\t\\t\\t[WARNING] No data was parsed from page [\"+str(page)+\"] of file [\"+file_path+\"]\")\n",
    "    return df\n",
    "\n",
    "def clean_df(df, file_path):\n",
    "    try:\n",
    "        df.columns = regular_columns\n",
    "        # Extracting attributes: from files\n",
    "        match = re.search(r'_(\\d{4}\\d{2}\\d{2})\\.', file_path)\n",
    "        df[\"bill_date\"] = datetime.strptime(match.group(1), '%Y%m%d').date()\n",
    "        df[\"account_ref\"] = re.search(r'releve_(\\d+)_', file_path).group(1)\n",
    "    except Exception:\n",
    "        print(\"\\t\\t[WARNING] A problem was found cleaning page [\"+str(page)+\"] of file [\"+file_path+\"]\")\n",
    "        pass\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = 'data/pdf/'\n",
    "regular_pattern = r'_00050814111_'\n",
    "area_first = [386.412,28.609,734.926,568.843]\n",
    "area_first = [355.202,29.352,731.21,567.357] #with header\n",
    "\n",
    "area_middle = [166.454,28.609,619.745,567.357]\n",
    "columns_bounds = [77.654,130.414,413.535, 491.561]\n",
    "\n",
    "l = os.listdir(data_path)\n",
    "r = re.compile(regular_pattern)\n",
    "regular_files = list(filter(r.search, os.listdir(data_path)))\n",
    "\n",
    "dfs = []\n",
    "dfs_prob = []\n",
    "print(\"There are \" + str(len(regular_files)) + \" files in the folder.\")\n",
    "print(\"Out of these, \" + str(len(files)) + \" are being parsed\")\n",
    "\n",
    "files=regular_files\n",
    "for i, file in enumerate(files): \n",
    "    file_path = os.path.join(data_path, file)\n",
    "    print(\"[INFO] Parsing file \"+file_path+\". [\"+str(i)+\"/\"+str(len(files))+\"]...\")\n",
    "\n",
    "    pdf_pages = count_pages(file_path)\n",
    "    \n",
    "    if pdf_pages!=\"all\":\n",
    "        print(\"\\t[INFO] Individual page parsing. Total of \",str(pdf_pages),\" pages to parse\")\n",
    "        \n",
    "    for page in [i+1 for i in range(pdf_pages)]:\n",
    "        try:\n",
    "            if page == 1:\n",
    "                # first page, always same area\n",
    "                print(\"\\t\\t\\t[INFO] First page settings...\")\n",
    "                df_temp = parse_pdf(file_path, page, area = area_first, columns = columns_bounds)\n",
    "                dfs.append([df_temp])\n",
    "            elif page == pdf_pages:\n",
    "                # last page, guessing!\n",
    "                print(\"\\t\\t\\t[INFO] Last page settings...\")\n",
    "                df_temp = parse_pdf(file_path, page, columns = columns_bounds)\n",
    "                dfs[i].append(df_temp)\n",
    "            else :\n",
    "                # middle pages\n",
    "                print(\"\\t\\t\\t[INFO] Middle page settings...\")\n",
    "                df_temp = parse_pdf(file_path, page, area = area_middle, columns = columns_bounds)\n",
    "                dfs[i].append(df_temp)\n",
    "            \n",
    "        except Exception as e:\n",
    "            dfs_prob.append(df_temp)\n",
    "            print(\"\\t[WARNING] File:\"+file_path+\" skipped due to error: '\"+str(e)+\"'. Continuing...\")\n",
    "            print(\"\\t\\t[ERROR] \"+str(e)+\" Continuing...\")\n",
    "\n",
    "#df = pd.concat(dfs).reset_index()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs_ok = []\n",
    "dfs_df = []\n",
    "for file in dfs:\n",
    "    dfs_ok.append(pd.concat([x for x in file if x is not None]))\n",
    "    \n",
    "    \n",
    "for file in dfs_ok:\n",
    "    print(file.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs_ok)\n",
    "print(df.shape)\n",
    "# Getting rid of the parsed headers\n",
    "df = df[df.date!=\"Date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# converting amounts into integer values\n",
    "convert_amount(df, \"credit\")\n",
    "convert_amount(df, \"debit\")\n",
    "\n",
    "convert_amount(df, \"table_solde\", errors = \"coerce\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# concatenating text from extra-column to operations column\n",
    "df[\"table_solde_tmp_str\"] = df[\"table_solde_tmp\"][df[\"table_solde_tmp_int\"].isnull()]\n",
    "df[\"table_solde_tmp_str\"] = df[\"table_solde_tmp_str\"].fillna(value = \"\")\n",
    "df[\"operation_details\"] = df[\"operation_details\"].fillna(value = \"\")\n",
    "\n",
    "df[\"operation\"] = df[\"operation_details\"] + df[\"table_solde_tmp_str\"]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating definitive debit column: Attributing amounts from extra-column to debit column\n",
    "df[df[\"table_solde_tmp_int\"].notnull()] # TODO: raise error here when debit_final is not null, and there is also a value at debit_tmp_int\n",
    "\n",
    "df[\"debit_final\"] = np.where(df[\"table_solde_tmp_int\"].notnull(), \n",
    "                              df[\"table_solde_tmp_int\"],\n",
    "                              df[\"debit_tmp_int\"]\n",
    "                             )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating definitive credit column: No actions at the time\n",
    "df[\"credit_final\"] = df[\"credit_tmp_int\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ignoring last row of total movements\n",
    "df[df[\"operation\"]==\"TOTAUX DES MOUVEMENTS\"]\n",
    "df[~df[\"credit\"].isnull() & ~df[\"debit\"].isnull()]\n",
    "\n",
    "total_values = ~df[\"credit\"].isnull() & ~df[\"debit\"].isnull()\n",
    "total_tag = df[\"operation\"]==\"TOTAUX DES MOUVEMENTS\"\n",
    "\n",
    "df = df[~(total_values | total_tag)]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating date fields and converting to datetime format\n",
    "df[\"date_op\"] = df.loc[:,\"date\"].str.split(\" \").str.get(0)\n",
    "\n",
    "df[\"value_op\"] = df[\"date\"].str.split(\" \").str.get(1)\n",
    "# TODO: better manage the errors here. There are weird-non-date values\n",
    "df[\"date_op_dt\"] = pd.to_datetime(df[\"date_op\"], dayfirst=True, errors=\"coerce\")\n",
    "df[\"value_op_dt\"] = pd.to_datetime(df[\"value_op\"], dayfirst=True, errors=\"coerce\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning columns\n",
    "df.drop([\"operation_details\", \n",
    "         \"debit\", \"debit_tmp\", \"debit_tmp_int\",\n",
    "         \"credit\", \"credit_tmp\", \"credit_tmp_int\",\n",
    "         \"table_solde\", \"table_solde_tmp\", \"table_solde_tmp_str\", \"table_solde_tmp_int\",\n",
    "         \"date\", \"value\", \"date_op\", \"value_op\"         \n",
    "        ], axis=1, inplace = True)\n",
    "\n",
    "df.rename(columns = {\"debit_final\" : \"debit\",\n",
    "                     \"credit_final\" : \"credit\",\n",
    "                     \"date_op_dt\" : \"date\",\n",
    "                     \"value_op_dt\" : \"value\",          \n",
    "          }, inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cleaning rows\n",
    "status_pattern = r'^\\*\\*\\* SOLDE'\n",
    "df = df[~df.operation.str.contains(status_pattern)]\n",
    "\n",
    "# Filtering all NaN rows\n",
    "df.operation = df.operation.replace('', np.nan)\n",
    "df.dropna(axis='index', how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfcopy=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Re-arranging rows and columns\n",
    "transaction_mask = ~(df.debit.isnull() & df.credit.isnull())\n",
    "transaction_index = pd.to_numeric(df.loc[transaction_mask,:].index)\n",
    "details_index = pd.to_numeric(df.loc[~transaction_mask,:].index)\n",
    "\n",
    "df[\"transaction_index\"]= np.nan\n",
    "df.loc[transaction_index, \"transaction_index\"] = transaction_index\n",
    "df[\"transaction_index\"]=df.transaction_index.fillna(method='ffill')\n",
    "\n",
    "df[\"operation_detail\"]=np.nan\n",
    "df.loc[details_index, \"operation_detail\"] = df.loc[details_index, \"operation\"]\n",
    "\n",
    "operation_details=df[~df.operation_detail.isnull()].groupby('transaction_index')['operation_detail'].apply(lambda x: '|'.join(x))\n",
    "df['op_details']=operation_details\n",
    "\n",
    "df_compact = df[transaction_mask].reset_index().drop([\"index\", \"transaction_index\", \"operation_detail\"], axis='columns')\n",
    "df_compact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_types = {\n",
    "    \"transfer\" : \"VIR\",\n",
    "    \"check\" : \"CHEQUE\",\n",
    "    \"charge\" : \"PRELEVEMENT\",\n",
    "    \"\" : \"COTISATION\",\n",
    "    \"\" : \"DEBIT\",\n",
    "    \"\" : \"ARRETE\",\n",
    "    \"\" : \"FRAIS\",\n",
    "    \"\" : \"CARTE\",\n",
    "}\n",
    "\n",
    "df_compact[\"op_type\"] = df_compact.operation.str.extract(r'(?P<numbers>[0-9 ]*)(?P<type>[A-Z]*)').loc[:,\"type\"]\n",
    "df_compact.op_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracting attributes: from operation details\n",
    "df_compact[\"from\"] = df_compact.op_details.str.extract(r'DE:(?P<de>[^|]+)')\n",
    "df_compact[\"id\"] = df_compact.op_details.str.extract(r'ID:(?P<de>[^|]+)')\n",
    "df_compact[\"to\"] = df_compact.op_details.str.extract(r'POUR:(?P<de>[^|]+)')\n",
    "df_compact[\"ref\"] = df_compact.op_details.str.extract(r'REF:(?P<de>[^|]+)')\n",
    "df_compact[\"motive\"] = df_compact.op_details.str.extract(r'MOTIF:(?P<de>[^|]+)')\n",
    "df_compact[\"at\"] = df_compact.op_details.str.extract(r'CHEZ:(?P<de>[^|]+)')\n",
    "df_compact[\"mandat\"] = df_compact.op_details.str.extract(r'MANDAT:(?P<de>[^|]+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
